# -*- coding: utf-8 -*-
"""BuildingsMaterials.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19NleMQ6eMdDgJ9E_iI_9o6_VUNmLUmBN
"""
import shutil
import sys
# подключение к google disk
#from google.colab import drive
#drive.mount ('/content/drive')

#text_file = "/content/drive/MyDrive/Datasets/output1.txt"
#text_file = "/content/drive/MyDrive/output1.txt"

text_file = "output_changed4.txt"
#text_file = "/content/translator_changed.txt"

with open(text_file, encoding='utf-8') as f:
        shutil.copyfileobj(f, sys.stdout)
        lines = f.read().split("\n")[:-1]
print(lines)

text_pairs = []
for line in lines:
    input, output = line.split("\t")
    output = "[start] " + output + " [end]"
    text_pairs.append((input, output))

import random
#print (random.choice(text_pairs))

import random

random.shuffle(text_pairs)
num_val_samples = int(0.15*len(text_pairs))
num_train_samples = len(text_pairs) -2  * num_val_samples
train_pairs = text_pairs[:num_train_samples]
val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]
test_pairs = text_pairs[num_train_samples + num_val_samples:]

print('Всего пар', len(text_pairs))
print('Пар для обучения', len(train_pairs),'  Пар для валидации', len(val_pairs),'  Пар для тестирования', len(test_pairs))

first_elements = [item[0] for item in train_pairs]
print (first_elements)

second_elements = [item[1] for item in train_pairs]
print (second_elements)

"""6. Векторизация пар"""

import tensorflow as tf
import string
import re
from tensorflow import keras
from tensorflow.keras import layers

strip_chars = string.punctuation + ",.!?"
strip_chars = strip_chars.replace("[", "")
strip_chars = strip_chars.replace("]", "")

def custom_standardization(input_string):
    lowercase = tf.strings.lower(input_string)
    return tf.strings.regex_replace(
        lowercase, f"[{re.escape(strip_chars)}]", "")

vocab_size = 15000
sequence_length = 20

input_vectorization = layers.TextVectorization(
    max_tokens = vocab_size,
    output_mode = "int",
    output_sequence_length = sequence_length,
)

output_vectorization = layers.TextVectorization(
    max_tokens = vocab_size,
    output_mode = "int",
    output_sequence_length = sequence_length + 1,
    standardize = custom_standardization,
)
train_input_texts = [pair[0] for pair in train_pairs]
train_output_texts = [pair[1] for pair in train_pairs]

input_vectorization.adapt(train_input_texts)
output_vectorization.adapt(train_output_texts)

input_voc = input_vectorization.get_vocabulary()
output_voc = output_vectorization.get_vocabulary()

print('Длина входного словаря', len(input_voc),input_voc)
print('Длина выходного словаря', len(output_voc),output_voc)

#rnd_index = random.randint(0, len(input_voc))
rnd_index = random.randint(0, 77)
#rnd_index = random.randint(0, 20)
print(rnd_index)

rnd_input = train_input_texts[rnd_index].strip(",.!?")

print(rnd_input)

words = rnd_input.split(' ')
print(words)

tokens = []

for word in words:
  tokens.append(input_voc.index(word))

print(words,"\t",tokens)

from tensorflow.python.types.core import runtime_checkable
batch_size = 64

def format_dataset(input, output):
    input = input_vectorization(input)
    output = output_vectorization(output)
    return({
        "input": input,
        "output": output[:, :-1],
    }, output[:, 1:])

def make_dataset(pairs):
  input_texts, output_texts = zip(*pairs)
  input_texts = list(input_texts)
  output_texts = list(output_texts)
  dataset = tf.data.Dataset.from_tensor_slices((input_texts,output_texts))
  dataset = dataset.batch(batch_size)
  dataset = dataset.map(format_dataset, num_parallel_calls = 4)
  return dataset.shuffle(2048).prefetch(16).cache()

train_ds = make_dataset(train_pairs)
val_ds = make_dataset(val_pairs)

for inputs, targets in train_ds.take(1):
    #print(inputs)
    print(f"inputs['input'].shape: {inputs['input'].shape}")
    print(f"inputs['output'].shape: {inputs['output'].shape}")
    print(f"targets.shape: {targets.shape}")

from tensorflow import keras
from tensorflow.keras import layers
#from tensorflow.python.keras.layers import Dense

embed_dim = 512
latent_dim = 1024

source = keras.Input(shape = (None,), dtype = "int64", name = "input")
x = layers.Embedding(vocab_size, embed_dim, mask_zero = True)(source)
encoded_source = layers.Bidirectional(
    layers.GRU(latent_dim), merge_mode = "sum")(x)

past_target = keras.Input(shape = (None,), dtype = "int64", name = "output")
x = layers.Embedding(vocab_size, embed_dim, mask_zero = True)(past_target)
decoder_gru = layers.GRU(latent_dim, return_sequences = True)
x = decoder_gru(x, initial_state = encoded_source)
#x = layers.Dropout(0.5)(x)
target_next_step = layers.Dense(vocab_size, activation = "softmax")(x)
seq2seq_rnn = keras.Model([source, past_target], target_next_step)

seq2seq_rnn.compile(
    optimizer = "rmsprop",
    loss = "sparse_categorical_crossentropy",
    metrics = ["accuracy"])

history = seq2seq_rnn.fit(train_ds, epochs = 95, validation_data=val_ds) # 300 = прямая линия

import numpy as np

output_vocab = output_vectorization.get_vocabulary()
output_index_lookup = dict(zip(range(len(output_vocab)),output_vocab))
max_decoded_sentence_length = 20

def decode_rnn_sequence(input_sentence):
  tokenized_input_sentence = input_vectorization([input_sentence])
  decoded_sentence = "[start]"
  for i in range(max_decoded_sentence_length):
    tokenized_output_sentence = output_vectorization([decoded_sentence])
    next_token_predictions = seq2seq_rnn.predict(
        [tokenized_input_sentence, tokenized_output_sentence],verbose = None)
    sampled_token_index = np.argmax(next_token_predictions[0, i, :])
    sampled_token = output_index_lookup[sampled_token_index]
    decoded_sentence += " " + sampled_token
    if sampled_token == "[end]":
      break
  return decoded_sentence

test_input_texts = [pair[0] for pair in test_pairs]

for _ in range (10):
  input_sentence = random.choice(test_input_texts)
  print("-")
  print(input_sentence)
  print(decode_rnn_sequence(input_sentence))

print(decode_rnn_sequence('Расскажи о именах материалах'))

#!pip install keras-nlp
#!pip install rouge

import keras_nlp
import rouge
from rouge import Rouge
from nltk.translate.bleu_score import sentence_bleu

sentences_bleu_test = []
sentences_rouge_test = []

N = 10

first_elements = [item[0] for item in random.sample(train_pairs, N)]
second_elements = [item[1] for item in random.sample(train_pairs, N)]

num_epoch = 30

class BRCallback(tf.keras.callbacks.Callback):
  def __init__(self):
    super(BRCallback, self).__init__()

  def compute_bleu(self):
    common_bleu = 0

    for i in range(len(first_elements)):
      input_sentence = second_elements[i]
      translated = decode_rnn_sequence(first_elements[i])
      bleu_score = sentence_bleu([input_sentence], translated) #,weights = (1, 0, 0, 0))
      common_bleu += bleu_score

    return common_bleu / len(first_elements)

  def compute_rouge(self):
    common_rouge = 0

    for i in range (len(first_elements)):
      input_sentence = second_elements[i]
      translated = decode_rnn_sequence(first_elements[i])
      rouge = Rouge()
      common_rouge +=rouge.get_scores(input_sentence, translated)[0]["rouge-1"]["f"]

    return common_rouge / len(first_elements)

  def on_epoch_end(self, epoch, logs = {}):
    common_test_bleu = self.compute_bleu()
    sentences_bleu_test.append(common_test_bleu)
    print('\nTrain BLEU на эпохе {0} - {1}'. format(epoch+1, common_test_bleu))

    common_test_rouge = self.compute_rouge()
    sentences_rouge_test.append(common_test_rouge)
    print('Train ROUGE на эпохе {0} - {1}'.format(epoch+1, common_test_rouge))

"""12. Обучение модели"""

seq2seq_rnn.compile(
    optimizer = "rmsprop",
    loss = "sparse_categorical_crossentropy",
    metrics = ["accuracy"])

history = seq2seq_rnn.fit(train_ds, epochs = 50, validation_data = val_ds, callbacks = (BRCallback())) # Было 200 эпох

"""13. Визуализация графиков"""

# Commented out IPython magic to ensure Python compatibility.
import keras
import matplotlib.pyplot as plt
# %matplotlib inline

fig, axis = plt.subplots(nrows = 1, ncols = 2,  figsize = (20,4))
fig.subplots_adjust(bottom = 0.3, top = 0.8, hspace = 2.0)
fig.tight_layout()

axis[0].plot(history.history['accuracy'])
axis[0].plot(history.history['val_accuracy'])
axis[0].set_title('accuracy')

axis[1].plot(history.history['loss'])
axis[1].plot(history.history['val_loss'])
axis[1].set_title ('loss')